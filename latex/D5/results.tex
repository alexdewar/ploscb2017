\section{Results}
In order to assess whether the R2 and R4 ring neurons represent a bottleneck for task-dependent information processing, a first step is to see whether the visual input experienced during tasks when filtered through these cells retains sufficient information to perform the specified task. We therefore first examined whether the output of populations of \acp{RF} could qualitatively replicate results from the behavioural literature, and next assessed what information is preserved about visual stimuli in the population code. 

In all cases, the output of the cell is taken to be: the output of the visual input which would be perceived by a fly at a particular position and orientation, passed through an averaged filter derived from the receptive fields in \cite{Seelig2013}.
An averaged filter is generated for each of the R2 and R4d glomeruli, by summing the \acp{RF} for individual flies after they have been centred over the same point (Fig.~\ref{fig:averagekern} and see \emph{Materials and Methods}).

\subsection{Replication of behavioural experiments}
In this section we assess whether there is sufficient information in activity of the ring neuron filters to carry out two behavioural tasks known to be performed by flies. The two tasks we focused on were bar and edge fixation \cite{Neuser2008,Osorio1990} and pattern discrimination \cite{Pan2009,Liu2006,Ernst1999}.

\subsubsection{Bar Fixation}
It is known that \emph{Drosophila} approach large vertical objects \cite{Reichardt1969}, as a means of finding a fruit tree on which to feed and lay eggs.
R4d, but not R2, neurons in particular are known to play a role in spatial orientation memory for bars, though homing towards vertical objects is mediated by Reichardt detectors \cite{Reichardt1969}.
Though the inputs to R4d neurons could include idiothetic information and there could be outputs to motor neurons, we would still expect that, as these cells are involved in a bar orientation task, they would provide enough information to an agent for bar homing.
To assess the ability of R2 and R4d neurons to fixate bars we simulated the visual input that would be experienced by a fly approaching vertical bars of different widths and examined the activity provoked (Fig.~\ref{fig:bar}A).

Although the ring neuron filters clearly respond to the bars, the pattern of results is somewhat complex.
For the narrowest bar, the R4d filters display large peaks in output at $\pm 120\degree$ from the bar, with smaller peaks at the centre, whereas the R2 filters display peaks within $\pm 90$ of the bar.
For the others, the peaks are at the edges of or inside the bars.
which is the location to which flies orient in behavioural observations \cite{Osorio1990}.
It is unclear how this information is translated into behaviour, but even a simple summation of the outputs of all neurons used to drive rotational velocity in a simulated fly, provides sufficient information for navigation towards a prominent bar, as we show in Fig.~\ref{fig:bar}B and C.
While these examples are simulations of very simple worlds as seen in many behavioural experiments, the cells also show increased activity to bar-like objects in natural scenes (Fig.~\ref{fig:bar}D).

\begin{comment}
To assess the ability of R2 and R4d neurons to fixate bars we simulated the visual input that would be experienced by a fly approaching vertical bars of different widths (OR: in  a number of situations) and examined the activity provoked. It is known that flies approach …  In the first instance we assumed the fly was at a fixed distance from the bars and examined mean activity for different azimuthal orientations/facing directions. [Both R2 and R4d neurons showed interesting responses (Fig.~\ref{fig:bar}A).] The R4d neurons, which are known to play a role in spatial orientation memory for bars \cite{Neuser2008}, showed peaks in activation at $\pm 90\degree$ from the bar's centre, the heading at which an agent performing bar fixation would saccade the most.
The R2 neurons, however, showed peaks towards the inside edges of the largest bars, which is the location to which flies orient in behavioural observations \cite{Osorio1990}. It is unclear how this information is translated into behaviour, but even a simple summation of the outputs of all neurons used to drive rotational velocity in a simulated fly, provides sufficient information for navigation towards a prominent bar [do we get avoidance? do flies show avoidance?] (Fig.~\ref{fig:bar}B and C). While these examples are simulations of very simple worlds as seen in many behavioural experiments, the cells also show increased activity to bar-like objects in natural scenes. As we see in Fig.~\ref{fig:bar}D, …. [Figure messy, tidy or find better ex]
\end{comment}

\subsubsection{Pattern Recognition}
The standard paradigm for testing pattern discrimination in \emph{Drosophila} \cite{Pan2009,Liu2006,Ernst1999,Dill1993}, involves tethering a fly in a drum with alternating patterns on the inside (Fig.~\ref{fig:recap}A).
When the fly attempts to rotate about the yaw-axis, the pattern on the drum is rotated by a corresponding amount in the opposite direction, giving closed-loop control.
Conditioning is aversive: Fixation upon certain portions of one of the patterns is punished with heat from a laser.
Hence, if the fly can discriminate the patterns, it will orient towards the non-punished pattern.

To recreate the visual information perceived by flies in such experiments, we simulated a typical experimental flight arena with a fly tethered in the centre.
We examined the difference in the output of R2 filters between patterns for each pattern pair (Fig.~\ref{fig:recap}B and C, see \emph{Materials and Methods} for details); the greater the difference, the more discriminable the patterns are for these neurons.
The pairs of patterns we used were drawn from \cite{Ernst1999}.
We have numbered pattern pairs according to the figure in which they appear in \cite{Ernst1999}, e.g. Set~\emph{(2)} refers to the patterns shown in Fig.~2 of that work (see Fig.~\ref{fig:pattern}).
We found a significant correlation between the strength of the learning index in Ernst and Heisenberg \cite{Ernst1999} and the difference we found in R2 activation (Spearman's rank, $n=34, \rho=.615, p<.005$).
There is also an older hypothesis that \emph{Drosophila} pattern discrimination is based on retinotopic matching \cite{Dill1993}, which we examined for comparison (data not shown).
In this case, the proportion of retinal overlap was not significantly correlated with the flies' learning index for different pattern pairs (Spearman's rank, $n=34, \rho=.270, p=\mathrm{n.s.}$), suggesting that the outputs of R2 filters more closely approximate \emph{Drosophila} behaviour.
Interestingly, this implies that flies could reliably discriminate pairs of patterns without explicitly coding for the kinds of visual parameters commonly believed to underlie this discrimination \cite{Pan2009,Liu2006,Ernst1999}.
We next discuss specific pattern sets in detail.

If we turn to Fig.~\ref{fig:pattern}, Set~\emph{(2)}, we can see that these pattern pairs -- unlearnable by flies -- also give only small differences in outputs for the R2 filters.
This may seem surprising, given that these patterns appear so different from one another to our human eyes (and they are also very dissimilar if compared retinotopically).
However, if the placement and form of the RFs is considered, the reason for the failure becomes more obvious (see Supp.~Fig.~X).

Other pattern sets, which may appear fairly similar to us, are nonetheless readily discriminable by flies and the simulation.
For example, Set~\emph{(9)} contains pairs of `triangles' (either a filled equilateral triangle, or a long and short bar arranged on top of one another), one facing up and the other down.
They are aligned either along the top and bottom or about the vertical centre of mass.
Flies are able to discriminate the former, but not the latter types of stimuli \cite{Ernst1999}.
Likewise, the R2 outputs show a smaller relative difference for the triangles aligned about the centre of mass than not, yet do so without explicitly coding for this parameter.

Another interesting pattern pair, Set~\emph{(6)}, consisting of only one pair, a large and a small square, was readily discriminable by both flies and the R2 filters.
While it could be that flies are explicitly coding the size of the squares, the R2 filters do not and yet are able to distinguish them readily.
However, this does not mean that the information is not there implicitly (see below).

There are, however, some discrepancies.
The flies in \cite{Ernst1999} in a couple of cases were better at discriminating pairs of horizontal than vertical lines (Set~\emph{(3)} \emph{vs} Set~\emph{(4)}, and the pairs in Set~\emph{(12)}), whereas the R2 filters performed approximately as well on both.
This may be because while our R2 filters are being presented with static stimulus pairs, for the flies the patterns were moving around horizontally, as noted in \cite{Ernst1999}.

We have shown that flies' performance on a pattern discrimination task can be matched with a simple model of R2 population activity.
However, it is not necessarily the case that these cells evolved for the purpose of discriminating arbitrary visual stimuli, as in the experimental task.
We found that if additional R2-type RFs were added to new locations in the visual field, performance on the discrimination task improved (see Supp. Fig. Y).

\subsection{What information is preserved in these neurons?}
We were interested in discovering what properties of visual stimuli, while not explicitly coded for by the ring neurons, may nonetheless be implicitly conveyed in the population outputs.
To do this, we trained a series of neural networks to discriminate sets of randomly generated stimuli -- ellipse-like `blobs' -- on the basis of specific parameters (see \emph{Materials and Methods} for details).
The networks were given as inputs either raw images of the stimuli, or the outputs of the R2, R4d or R2 and R4d filters presented with the same stimuli.

We first looked at whether the neural networks could be trained to extract positional information about a stimulus: elevation and azimuth.
%To find out what information about stimuli is implicit in the outputs of R2 and R4 filters, we trained neural networks (see \emph{Materials and Methods}) to see whether they could extract positional information about visual stimuli, using raw views, R2 \ac{RF} values, R4d \ac{RF} values or R2 and R4d \ac{RF} values as inputs.
The stimuli used were ellipse-like `blobs', with orientation and major-axis length held constant ($\mathrm{orientation} = 0\degree, a = 30\degree$).
There were 100 possible azimuths and 100 possible elevations, giving a total of 10,000 stimuli.
Of these, 4000 were used for training and 6000 for testing.
Results are shown in Fig.~\ref{fig:elaz}.
The neural networks were indeed able to extract information about elevation and azimuth based on any of the input types.
Performance was better with parameter values nearer the middle, as at the extremes the stimuli lay partially outside the visual field.
Though overall performance was best with raw views, it was also good with the sets of ring neuron inputs, indicating that these ring neurons implicitly convey information about these parameters.

We next trained the same kind of networks to extract information about properties on the basis of which \emph{Drosophila} are known to be able to discriminate visual stimuli \cite{Pan2009,Liu2006,Ernst1999}: orientation, size and elevation.
The stimuli were again randomly generated ellipse-like blobs.
Ten different orientations, sizes and elevations were used, giving a total of 1000 stimuli, of which 400 were used for training and 600 for testing.
The networks were again able to extract information about orientation, size and elevation (Fig.~\ref{fig:orsiel}).
The `elevation' parameter, as with the previous experiment, shows poorer performance at the extremes.
`Orientation' was the parameter with the highest error scores, presumably because it represents a second-order property, unlike elevation and size.
Nonetheless, all three parameters could be simultaneously estimated by a neural network with ring neuron inputs, indicating that flies could be trained to distinguish arbitrary stimuli differing along these parameters.

\subsection{[Things we haven't done yet, but could do]}
\begin{enumerate}
 \item Show tuning preferences for R2s and R4s. Could also show cell response varies over orientation and azimuth, e.g. with a heat map. At the least we could just describe the spread of preferred orientations (e.g. `the mean preferred orientation for R2s was $x\pm y\degree$').
 \item Show the triangles that are aligned about the vertical centre of mass vs. those that aren't with the R2 RFs overlaid. Then we could have a bar graph showing individual responses. This could be an alternative to the current random blobs figure.
 \item Show that pattern discriminability is better with more R2 RFs or with spread out R2s (our Rx cells) or both.
 \item In experiments flies are better at distinguishing pairs of horizontal than vertical lines, probably because they're moving from side to side in training and testing. We could simulate this by taking a weighted mean for each RF over e.g. $\pm 5\degree$ for the `line' patterns, and then comparing these values instead of just the straight activations. Wouldn't need to turn it into a figure or anything, but then we could mention in passing that we tried it and it worked.
 \item There's a paper showing that flies novelty preference for patterns involves the ring neurons, but no one subset specifically \cite{Solanki2015}. At the moment we're comparing R2 outputs for different patterns and this correlates with the learning indices given in Ernst and Heisenberg. We could throw the R4s into the mix and see if the results then correlate with the spontaneous pattern preferences given.
\end{enumerate}