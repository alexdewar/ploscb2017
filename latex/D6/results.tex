\section{Results}
R2 neurons have long been established to be involved in the processing of visual patterns \cite{Pan2009,Liu2006,Ernst1999}.
The R4d cells, although not critical for discrimination tasks, have been implicated in a task involving a learned heading relative to a pattern \cite{Guo2015} and, along with other ring neurons, novelty detection for visual patterns \cite{Solanki2015}.
Strikingly, \emph{Drosophila} are able to perform these tasks with only a tiny number of such neurons (R2: 28 \cite{Seelig2013}; R4d: 14 \cite{Seelig2013}; total ring neurons: approx. 63\cite{MartinPena2014}).
Early work into visual pattern discrimination in \emph{Drosophila} suggested that flies were comparing patterns on the basis of retinotopic overlap \cite{Dill1995,Dill1993}.
This model, although offering predictive power for a limited pattern subset, was shown to be incapable of discriminating other patterns that were however discriminable by flies \cite{Ernst1999} and it is now generally assumed instead that flies are encoding the patterns on the basis of certain higher-order features, viz. size, orientation, elevation and vertical compactness \cite{Ernst1999,Liu2006,Pan2009}.
Here we refer to these two hypotheses as the `Retinotopic Model' and the `Cognitive Model', respectively.
More recently, work by Seelig and Jayaraman \cite{Seelig2013} showing the RFs of the critically important R2 neurons presents an intriguing third possibility, namely that discrimination is performed on the basis of simple differences in outputs of the R2 cells; we have termed this the `RF Model'.
We feel this approach has the advantage of remaining parsimonious whilst still yielding strong predictive power.
It is also compatible with the finding that at least some of the learning in these tasks takes place at the level of R2 synapses \cite{Wang2008}.

There are two separate aspects to the ring neuron outputs that we wished to examine.
First, we wanted to compare the Retinotopic and RF Models of pattern discrimination, which, unlike the Cognitive Model, make no assumptions about higher-order processing of stimuli.
Here we found that the output of the RF Model, but not of the Retinotopic Model was significantly correlated with flies' ability to learn pattern distinctions.
This indicates that a simple mechanism could underlie pattern discrimination, with no higher-order processing required.

Second, we wanted to know what kind of visual information passes through the narrow bottleneck given by this small number of cells.
For this, we trained a series of \acp{ANN} to recover various higher-order properties from randomly generated stimuli, with input from either raw pixel information ($N=28$), or from the R2, R4d or R2 and R4d filters.
This showed that, despite the lack of specific encoding for any one of these parameters, nonetheless they are to an extent implicitly encoded in the outputs and hence could be extracted by a neural mechanism upstream.

\subsection{Pattern Recognition}
We first compared two models of visual pattern discrimination in \emph{Drosophila} -- the Retinotopic Model \cite{Dill1995,Dill1993} and our own RF Model -- using a simulation of a standard behavioural paradigm.

The standard paradigm for testing pattern discrimination in \emph{Drosophila} \cite{Pan2009,Liu2006,Ernst1999,Dill1993}, involves tethering a fly in a drum with alternating patterns on the inside (Fig.~\ref{fig:recap}A).
When the fly attempts to rotate about the yaw-axis, the pattern on the drum is rotated by a corresponding amount in the opposite direction, giving closed-loop control.
Conditioning is aversive: Fixation upon certain portions of one of the patterns is punished with heat from a laser.
Hence, if the fly can discriminate the patterns, it will orient towards the non-punished pattern.

To recreate the visual information perceived by flies in such experiments, we simulated a typical experimental flight arena with a fly tethered in the centre.
We examined the difference in the output of R2 filters between patterns for each pattern pair (Fig.~\ref{fig:recap}B and C, see \emph{Materials and Methods} for details); the greater the difference, the more discriminable the patterns are for these neurons.
The pairs of patterns we used were drawn from \cite{Ernst1999}.
We have numbered pattern pairs according to the figure in which they appear in \cite{Ernst1999}, e.g. Set~\emph{(2)} refers to the patterns shown in Fig.~2 of that work (see Fig.~\ref{fig:pattern}).
We found a significant correlation between the strength of the learning index in Ernst and Heisenberg \cite{Ernst1999} and the difference we found in R2 activation (Spearman's rank, $n=34, \rho=.615, p<.005$).
By contrast, the proportion of retinal overlap was not significantly correlated with the flies' learning index for different pattern pairs (Spearman's rank, $n=34, \rho= -0.215, p=\mathrm{n.s.}$), suggesting that the outputs of R2 filters more closely approximate \emph{Drosophila} behaviour.
Interestingly, this implies that flies could reliably discriminate pairs of patterns without explicitly coding for the kinds of visual parameters commonly believed to underlie this discrimination \cite{Pan2009,Liu2006,Ernst1999}.
We next discuss specific pattern sets in detail.

If we turn to Fig.~\ref{fig:pattern}, Set~\emph{(2)}, we can see that these pattern pairs -- unlearnable by flies -- also give only small differences in outputs for the R2 filters.
This may seem surprising, given that these patterns appear so different from one another to our human eyes (and they are also very dissimilar if compared retinotopically).
However, if the placement and form of the RFs is considered, the reason for the failure becomes more \hltodo{obvious}{Explain the obvious} (see Fig.~\ref{fig:simdiffpatts}).

Other pattern sets, which may appear fairly similar to us, are nonetheless readily discriminable by flies and the simulation.
For example, Set~\emph{(9)} contains pairs of `triangles' (either a filled equilateral triangle, or a long and short bar arranged on top of one another), one facing up and the other down.
They are aligned either along the top and bottom or about the vertical centre of mass.
Flies are able to discriminate the former, but not the latter types of stimuli \cite{Ernst1999}.
Likewise, the R2 outputs show a smaller relative difference for the triangles aligned about the centre of mass than not, yet do so without explicitly coding for this parameter.

Another interesting pattern pair, Set~\emph{(6)}, consisting of only one pair, a large and a small square, was readily discriminable by both flies and the R2 filters.
While it could be that flies are explicitly coding the size of the squares, the R2 filters do not and yet are able to distinguish them readily.
However, this does not mean that the information is not there implicitly (see below).

There are, however, some discrepancies.
The flies in \cite{Ernst1999} in a couple of cases were better at discriminating pairs of horizontal than vertical lines (Set~\emph{(3)} \emph{vs} Set~\emph{(4)}, and the pairs in Set~\emph{(12)}), whereas the R2 filters performed approximately as well on both.
This may be because while our R2 filters are being presented with static stimulus pairs, for the flies the patterns were moving around horizontally, as noted in \cite{Ernst1999}.
\todo{“To extend this we can design shapes that look different but are not...”}

We have shown that flies' performance on a pattern discrimination task can be matched with a simple model of R2 population activity.
However, it is not necessarily the case that these cells evolved for the purpose of discriminating arbitrary visual stimuli, as in the experimental task.
In fact, a simple test -- either evenly spacing the RFs or adding RFs -- showed that improved performance on this task could have been achieved easily by evolution (data not shown).
This suggests that R2 neurons evolved either for a different, specific task, or as more general purpose mechanism for operant conditioning (see Discussion).

\subsection{What information is preserved in these neurons?}
We were also interested in discovering what properties of visual stimuli, while not explicitly coded for by the ring neurons, may nonetheless be implicitly conveyed in the population outputs.
To do this, we trained a series of neural networks to discriminate sets of randomly generated stimuli -- ellipse-like `blobs' -- on the basis of specific parameters (see \emph{Materials and Methods} for details).
The networks were given as inputs either raw images of the stimuli, or the outputs of the R2, R4d or R2 and R4d filters presented with the same stimuli.

We first looked at whether the neural networks could be trained to extract positional information about a stimulus: elevation and azimuth.
The stimuli used were ellipse-like `blobs', with orientation and major-axis length held constant ($\mathrm{orientation} = 0\degree, a = 30\degree$).
There were 100 possible azimuths and 100 possible elevations, giving a total of 10,000 stimuli.
Of these, 4000 were used for training and 6000 for testing.
Results are shown in Fig.~\ref{fig:elaz}.
The neural networks were indeed able to extract information about elevation and azimuth based on any of the input types.
Performance was better with parameter values nearer the middle, as at the extremes the stimuli lay partially outside the visual field.
Though overall performance was best with raw views, it was also good with the sets of ring neuron inputs, indicating that these ring neurons implicitly convey information about these parameters.

We next trained the same kind of networks to extract information about properties on the basis of which \emph{Drosophila} are known to be able to discriminate visual stimuli \cite{Pan2009,Liu2006,Ernst1999}: orientation, size and elevation.
The stimuli were again randomly generated ellipse-like blobs.
Ten different orientations, sizes and elevations were used, giving a total of 1000 stimuli, of which 400 were used for training and 600 for testing.
The networks were again able to extract information about orientation, size and elevation (Fig.~\ref{fig:orsiel}).
The `elevation' parameter, as with the previous experiment, shows poorer performance at the extremes.
`Orientation' was the parameter with the highest error scores, presumably because it represents a second-order property, unlike elevation and size.
Nonetheless, all three parameters could be simultaneously estimated by a neural network with ring neuron inputs, indicating that flies could be trained to distinguish arbitrary stimuli differing along these parameters.

%\subsection{Summary}
In summary, we have shown that information about a number of higher-order properties, taken from the Cognitive Model, passes through the bottleneck of this small number of neurons.
This indicates that such information could be used in some way upstream of the ring neurons, perhaps as an explicit encoding, although the information could also be used implicitly.
However, as we have shown in the previous section, in order to perform a pattern discrimination task, such an explicit coding is not necessary.
Likewise, presumably there are other tasks in which R2 neurons play a role where extraction of these parameters is either not necessary or would even hinder performance.