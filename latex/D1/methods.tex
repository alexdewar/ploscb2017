
\section{Methods}
\subsection{Pre-processing the receptive fields}
\label{sec:methods:preprocessing}
The \ac{RF} data were drawn from \citeA{Seelig2013} (Extended Data Figure 8), which comprised measurements from 7 R2 glomeruli and 14 R4d glomeruli in the lateral triangle; the precise number of flies included varied by glomerulus ($2\le N\le 7$).
We first processed the data into a form suitable for use in our simulations.
Here, as throughout, Matlab\textregistered\ (MathWorks, Natick, MA, USA) was used to perform all calculations.

The data were in the form of images, with red areas for excitatory regions and blue for inhibitory.
Each point on the image was assigned a value ranging from --1 to 1, representing maximum inhibition and excitation, respectively.
These values were then thresholded:
$$
g_{i,j} = [I_{i,j} \ge \Theta] - [I_{i,j} \le -\Theta]
%g_{i,j} = \frac{1}{2} ( [I_{i,j} \ge \Theta] - [I_{i,j} \le -\Theta] + 1 )
$$
where $g_{i,j}$ is the ($i,j$)th pixel of the thresholded kernel, $I_{i,j}$ is the ($i,j$)th value of the processed receptive field image and $\Theta$ is the threshold value, here $0.25$.

The centroid for the largest excitatory region, at coordinates $(x,y)$, is calculated using Matlab's \texttt{regionprops} function on an image with values of 1 for positive values and 0 otherwise.
The mean centroid, $(\bar{x},\bar{y})$, across flies is also calculated, and the kernels are recentred to these coordinates:
$$
g_{i,j} := \left\{ \begin{array}{ll} g_{i+y-\bar{y},j+x-\bar{x}} & \mbox{for } 1\le i+y-\bar{y}\le m \mbox{ and } 1\le j+x-\bar{x}\le n;\\
0 & \mbox{otherwise.} \end{array} \right.
$$

We next calculate the average \ac{RF} across flies, $\bar{g}_{i,j}$, and threshold again:
$$
\bar{g}_{i,j} = \frac{1}{2}([c \ge \Theta] - [c \le -\Theta] + 1), c = \frac{1}{|\bm{G}|}\sum\limits_{g \in \bm{G}} g_{i,j}
$$
where $\bm{G}$ is the set of kernels being averaged and $\Theta$ is the threshold (again: 0.25).
In this case, however, the \ac{RF} is given a value of 1 for excitation, 0 for inhibition and $\frac{1}{2}$ for no activity.

In order to calculate the activation for a given \ac{RF} on presentation of an image, $I$, the \ac{RF} must first be resized to the same size as the image.
This is accomplished by resizing the average \ac{RF}, $\bar{g}_{i,j}$, with Matlab's \texttt{imresize} function to the same dimensions as $I$.
Finally, the kernel is rethresholded and the excitatory and inhibitory regions are assigned different values:
$$
K_{i,j} = \left\{
\begin{array}{rl}
\left( \sum\limits^m_{i=1}\sum\limits^n_{j=1}[\bar{g}_{i,j} \ge \frac{1+\Theta}{2}] \right)^{-1}, & \mbox{for } \bar{g}_{i,j} = 1; \\
-\left( \sum\limits^m_{i=1}\sum\limits^n_{j=1}[\bar{g}_{i,j} \le \frac{1-\Theta}{2}] \right)^{-1}, & \mbox{for } \bar{g}_{i,j} = 0; \\
0, & \mbox{otherwise.}
\end{array}
\right.
$$
This method of allocating values has the result that $\sum\limits_{i=1}^m \sum\limits_{j=1}^n K_{i,j}=1$.

\begin{comment}
The total numbers of excitatory, $N_{\mathrm{exc}}$, and inhibitory values, $N_{\mathrm{inh}}$, for the average kernel are then calculated:
\begin{align*}
N_{\mathrm{exc}} &= \sum\limits_{i=1}^m \sum\limits_{j=1}^n [K_{i,j} = 1] \\
N_{\mathrm{inh}} &= \sum\limits_{i=1}^m \sum\limits_{j=1}^n [K_{i,j} = -1]
\end{align*}
\end{comment}

The activation of an average kernel, $K$, to the presentation of an image stimulus, $I$, is then:
$$
\begin{array}{rl}
A(I,K) = {\sum\limits^m_{i=1} \sum\limits^n_{j=1} I_{i,j}K_{i,j}}, &\mathrm{where\ } 0 \le I_{i,j} \le 1
\end{array}
$$
where $I_{i,j}$ and $K_{i,j}$ are the ($i,j$)th pixels of the image and kernel, respectively. %This means for an image where every pixel $I_{i,j}=1$ or $I_{i,j}=0$, $A(I,K) = 1$.

\subsection{Can we replicate behavioural experiments?}
We first examined whether we could qualitatively replicate results from the behavioural literature.
The two tasks we focused on were bar and edge fixation \cite{Neuser2008,Osorio1990} and pattern discrimination \cite{Liu2006,Ernst1999}.

Figure~\ref{fig:recap}A~\&~B shows the population response of R4d neurons to bars of two different widths.
The population response was calculated by taking the mean of the individual activations of the \acp{RF}.
Note that the difference between the activations of the left-hand and right-hand neurons would be sufficient to drive fixation to a vertical bar, a behaviour flies are known to engage in spontaneously [cit].

There is a paradigm for testing pattern recogntion in \emph{Drosophila} \cite{Liu2006,Ernst1999}, in which a tethered fly is placed into a drum with repeating visual patterns projected onto the inside (see Figure~\ref{fig:pattern} for examples).
When the fly attempts to rotate about the yaw-axis, the pattern on the drum is shifted by a corresponding amount in the opposite direction, giving the illusion of movement.
Conditioning is aversive: fixation upon certain portions of the pattern is punished with heat from a laser.
To test this, we used a modified version of the \ac{RIDF} \cite{Philippides2011,Zeil2003}.
In the standard version of the \ac{RIDF}, an image is recorded at a goal location and homing back to that location is achieved by repeatedly rotating on the spot and heading in the direction for which the \ac{rms} difference between current and goal views is at a minimum.
In our case, instead of raw pixel content of views, we used the activations of individual R2 neurons.
The `goal view' was taken to be the activation for the unrotated version of the pattern; the \ac{rms} difference between activation for rotated and unrotated views was then calculated (Figure~\ref{fig:recap}C--F):
$$
r(I,\theta,\bm{G}) = \frac{1}{|\bm{G}|\cdot mn} {\sum\limits_{K \in \bm{G}} \sum\limits^n_{j=1} \sum\limits^m_{i=1} (I_{i,j}(0)-I_{i,j}(-\theta))^2 \cdot {K_{i,j}}^2}
$$
where $r(I,\theta,\bm{G})$ indicates the \ac{rms} difference between the activations of the set of \acp{RF}, $\bm{G}$, for an image, $I$, and the same image at rotation $-\theta$ (i.e. opposite to the fly's direction of motion).

\subsection{What information is preserved in these neurons?}
Neural networks were used as a means of testing what properties of a visual stimulus can be encoded in a simple network, after `passing through' the ring-neuron \acp{RF}.
Two-layer feedforward networks with 10 hidden units, optimised with the scaled conjugate gradient function, were used throughout; these were implemented using the \texttt{Netlab} toolbox for Matlab \cite{netlab}.

\subsubsection{Stimuli}
\label{sec:methods:stimuli}
The stimuli with which the networks were trained were a series of black `blobs' on a white background.
The blobs were based on ellipses with a fixed ratio between the lengths of the major and minor axes ($2:1$), with the radii modified with complex waves:
$$
r \le \left(\frac{\cos^2 \theta}{2} + \frac{\sin^2 \theta}{a} \right)^{-1} + W(\theta)
$$
where $a$ is the length of the major axis and $W(\theta)$ is a complex wave defined as:
$$
W(\theta) = \sum_{i=1}^n W_i(\theta) = \sum_{i=1}^n A_i \sin f_i (\theta+\phi_i) 
$$
where $A_i$, $f_i$ and $\phi_i$ describe the maximum amplitude, frequency and phase shift of the wave $W_i(\theta)$, respectively.
In these experiments, $A_i$, $f_i$ and $\phi_i$ were randomly generated and $n=5$.

\subsubsection{Grading performance of neural networks}
The performance of the neural networks was evaluated using a mutual information metric \cite{Thomson2005}, which computes the amount of information (in bits) transmitted by a set of responses, $\mathcal{R}$, about a set of possible stimuli, $\mathcal{S}$.
One way of calculating this is by treating stimuli as discrete, rather than continuous variables.
Although the stimuli used here were already in discrete categories, the responses of the neural network were continuous, so these were divided into bins.
As the range of possible activations is not fixed as it is with the stimuli, the number of bins varied to accommodate this.
$$
I(\mathcal{S},\mathcal{R}) = H(\mathcal{S})-H(\mathcal{S},\mathcal{R}) = \sum\limits^n_{j=1} \sum\limits^m_{i=1} P(s_i,r_j)\times \log_2 \left( \frac{P(s_i,r_j)}{P(s_i)P(r_j)} \right)
$$
where $I(\mathcal{S},\mathcal{R})$ is the mutual information between $\mathcal{S}$ and $\mathcal{R}$, $H(\mathcal{S})$ is the entropy of $\mathcal{S}$ and $H(\mathcal{S},\mathcal{R})$ is the joint entropy of $\mathcal{S}$ and $\mathcal{R}$.
$n$ is the number of possible stimuli and $m$ is the number of response `bins'.
$P(s_i)$, $P(r_j)$ and $P(s_i,r_j)$ represent the probability of observing a stimulus, $s_i$, a response, $r_j$, and their joint probability, respectively.
We assumed the probabilities of different stimuli were equal, i.e. $P(s_i) = \frac{1}{m}$.

\subsubsection{Can the neural networks extract stimulus position?}
We first trained networks to see whether they could extract positional information about visual stimuli.
The stimuli used were the ellipse-like `blobs' (Section~\ref{sec:methods:stimuli}), with orientation and major-axis length held constant ($\mathrm{orientation} = 0, a = 30$).
There were 100 possible azimuths (equally spaced between --135\degree\ and 135\degree) and 100 possible elevations (equally spaced between --60\degree\ and 60\degree), giving a total of 10,000 stimuli.
Of these, 4000 were used for training and 6000 for testing.

Results are shown in Figure~\ref{fig:elaz}.

\subsubsection{What other properties can a network extract?}
We next attempted to train networks to extract information about properties on the basis of which \emph{Drosophila} are known to be able to discriminate visual stimuli \cite{Liu2006}: orientation, size and elevation.
As stimuli we used randomly generated blobs varying systematically along these parameters.
The size of the blob was determined on the basis of the length of the major axis, ignoring the `wave' component, using the standard equation for the area of an ellipse: $A = \pi ab$.
The orientation of the blob was altered by using the \texttt{imrotate} function in Matlab, after the blob had been centred on its centre of mass.
The azimuth, however, was held constant at the mean azimuth for the RFs centres (Section~\ref{sec:methods:preprocessing}) for the left-hand versions, --93\degree.
Ten different orientations, sizes and elevations were used, giving a total of 1000 stimuli, of which 400 were used for training and 600 for testing.

Results are shown in Figure~\ref{fig:orsiel}.

